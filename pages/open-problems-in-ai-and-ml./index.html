<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Open problems in AI and ML. | Shriman Keshri</title>
<meta name=keywords content="Reinforcement Learning"><meta name=description content="A blog contains 29 Open problems in AI and ML.
Explainability Learning to learn #Evolvebility Learning to learn or *meta-learning *(e.g., Harlow, 1949; Schmidhuber, 1987; Thrun and Pratt, 1998; Andrychowicz et al., 2016; Chen et al., 2016; de Freitas, 2016; Duan et al., 2016; Lake et al., 2016; Wang et al., 2016) is the acquisition of skills and inductive biases that facilitate future learning. The scenarios considered in particular are ones where a more general and slower learning process produces a faster, more specialized one."><meta name=author content="Shriman Keshri"><link rel=canonical href=https://shrimansoft.github.io/knowledge-base/pages/open-problems-in-ai-and-ml./><link crossorigin=anonymous href=https://shrimansoft.github.io/knowledge-base/assets/css/stylesheet.min.49b1c3611e5e823e974d0b9b3e1101617fce26c004757161abb1a1e4af3ff85c.css integrity="sha256-SbHDYR5egj6XTQubPhEBYX/OJsAEdXFhq7Gh5K8/+Fw=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=https://shrimansoft.github.io/knowledge-base/assets/js/highlight.min.30d2332871da51f600f574811c17751e6c862577d450b624f86e2bc8a6e31221.js integrity="sha256-MNIzKHHaUfYA9XSBHBd1HmyGJXfUULYk+G4ryKbjEiE=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://shrimansoft.github.io/knowledge-base/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shrimansoft.github.io/knowledge-base/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shrimansoft.github.io/knowledge-base/favicon-32x32.png><link rel=apple-touch-icon href=https://shrimansoft.github.io/knowledge-base/apple-touch-icon.png><link rel=mask-icon href=https://shrimansoft.github.io/knowledge-base/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.123.6"><link rel=alternate hreflang=en href=https://shrimansoft.github.io/knowledge-base/pages/open-problems-in-ai-and-ml./><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Open problems in AI and ML."><meta property="og:description" content="A blog contains 29 Open problems in AI and ML.
Explainability Learning to learn #Evolvebility Learning to learn or *meta-learning *(e.g., Harlow, 1949; Schmidhuber, 1987; Thrun and Pratt, 1998; Andrychowicz et al., 2016; Chen et al., 2016; de Freitas, 2016; Duan et al., 2016; Lake et al., 2016; Wang et al., 2016) is the acquisition of skills and inductive biases that facilitate future learning. The scenarios considered in particular are ones where a more general and slower learning process produces a faster, more specialized one."><meta property="og:type" content="article"><meta property="og:url" content="https://shrimansoft.github.io/knowledge-base/pages/open-problems-in-ai-and-ml./"><meta property="article:section" content="pages"><meta property="article:published_time" content="2024-03-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Open problems in AI and ML."><meta name=twitter:description content="A blog contains 29 Open problems in AI and ML.
Explainability Learning to learn #Evolvebility Learning to learn or *meta-learning *(e.g., Harlow, 1949; Schmidhuber, 1987; Thrun and Pratt, 1998; Andrychowicz et al., 2016; Chen et al., 2016; de Freitas, 2016; Duan et al., 2016; Lake et al., 2016; Wang et al., 2016) is the acquisition of skills and inductive biases that facilitate future learning. The scenarios considered in particular are ones where a more general and slower learning process produces a faster, more specialized one."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Pages","item":"https://shrimansoft.github.io/knowledge-base/pages/"},{"@type":"ListItem","position":2,"name":"Open problems in AI and ML.","item":"https://shrimansoft.github.io/knowledge-base/pages/open-problems-in-ai-and-ml./"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Open problems in AI and ML.","name":"Open problems in AI and ML.","description":"A blog contains 29 Open problems in AI and ML.\nExplainability Learning to learn #Evolvebility Learning to learn or *meta-learning *(e.g., Harlow, 1949; Schmidhuber, 1987; Thrun and Pratt, 1998; Andrychowicz et al., 2016; Chen et al., 2016; de Freitas, 2016; Duan et al., 2016; Lake et al., 2016; Wang et al., 2016) is the acquisition of skills and inductive biases that facilitate future learning. The scenarios considered in particular are ones where a more general and slower learning process produces a faster, more specialized one.","keywords":["Reinforcement Learning"],"articleBody":"A blog contains 29 Open problems in AI and ML.\nExplainability Learning to learn #Evolvebility Learning to learn or *meta-learning *(e.g., Harlow, 1949; Schmidhuber, 1987; Thrun and Pratt, 1998; Andrychowicz et al., 2016; Chen et al., 2016; de Freitas, 2016; Duan et al., 2016; Lake et al., 2016; Wang et al., 2016) is the acquisition of skills and inductive biases that facilitate future learning. The scenarios considered in particular are ones where a more general and slower learning process produces a faster, more specialized one. An example is biological evolution producing efficient learners such as human beings.\nTests Learning to play Atari video games is an area that has seen some remarkable recent successes, including in transfer learning (Parisotto et al., 2016). However, there is so far no system that first learns to play video games, then is capable of learning a new game, as humans can, from a few minutes of play (Lake et al., 2016). Knowing when you don’t know While uncertainty is modeled differently by different learning algorithms, it seems to be true in general that current artificial systems are not nearly as good as humans at “knowing when they don’t know.” An example are deep neural networks that achieve state-of-the-art accuracy on image recognition but assign 99.99% confidence to the presence of objects in images completely unrecognizable to humans (Nguyen et al., 2015). Human performance on confidence estimation would include\nIn induction tasks, like program induction or sequence completion, knowing when the provided examples are insufficient for induction (multiple reasonable hypotheses could account for them)\nIn speech recognition, knowing when an utterance has not been interpreted reliably\nIn visual tasks such as pedestrian detection, knowing when a part of the image has not been analyzed reliably\nTests A speech recognizer can be compared against a human baseline, measuring the ratio of the average confidence to the confidence on examples where recognition fails.\nThe confidence of image recognition systems can be tested on generated adversarial examples.\nLearning through action Human infants are known to learn about the world through experiments, observing the effects of their own actions (Smith and Gasser, 2005; Malik, 2015). This seems to apply both to higher-level cognition and perception. Animal experiments have confirmed that the ability to initiate movement is crucial to perceptual development (Held and Hein, 1963) and some recent progress has been made on using motion in learning visual perception (Agrawal et al., 2015). In (Agrawal et al., 2016), a robot learns to predict the effects of a poking action. “Learning through action” thus encompasses several areas, including\nActive learning, where the agent selects the training examples most likely to be instructive\nUndertaking epistemological actions, i.e., activities aimed primarily at gathering information\nLearning to perceive through action\nLearning about causal relationships through action\nPerhaps most importantly, for artificial systems, learning the causal structure of the world through experimentation is still an open problem. Tests For learning through action, it is natural to consider problems of motor manipulation where in addition to the immediate effects of the agent’s actions, secondary effects must be considered as well.\nLearning to play billiards: An agent with little prior knowledge and no fixed training data is allowed to explore a real or virtual billiard table and should learn to play billiards well.\nTransfer flexibility ","wordCount":"548","inLanguage":"en","datePublished":"2024-03-01T00:00:00Z","dateModified":"2024-03-01T00:00:00Z","author":{"@type":"Person","name":"Shriman Keshri"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shrimansoft.github.io/knowledge-base/pages/open-problems-in-ai-and-ml./"},"publisher":{"@type":"Organization","name":"Shriman Keshri","logo":{"@type":"ImageObject","url":"https://shrimansoft.github.io/knowledge-base/favicon.ico"}}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-00000XXXXX"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),-gtag("config","G-00000XXXXX")</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shrimansoft.github.io/knowledge-base/ accesskey=h title="Shriman Keshri (Alt + H)">Shriman Keshri</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://shrimansoft.github.io/knowledge-base/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://shrimansoft.github.io/knowledge-base/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://shrimansoft.github.io/knowledge-base/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://shrimansoft.github.io/knowledge-base/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><h1 class=post-title>Open problems in AI and ML.</h1><div class=post-meta><span title='2024-03-01 00:00:00 +0000 UTC'>March 1, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Shriman Keshri</div></header><div class=post-content><p><a href=https://omnivore.app/shrimansoft/unsolved-problems-in-ai-ai-forum-18df9724871>A blog contains 29 Open problems in AI and ML.</a></p><h2 id=explainability>Explainability<a hidden class=anchor aria-hidden=true href=#explainability>#</a></h2><h2 id=learning-tolearn-evolvebility>Learning to learn #Evolvebility<a hidden class=anchor aria-hidden=true href=#learning-tolearn-evolvebility>#</a></h2><ul><li><p><em>Learning to learn</em> or *meta-learning *(e.g., Harlow, 1949; <a href=http://people.idsia.ch/~juergen/diploma.html>Schmidhuber, 1987</a>; Thrun and Pratt, 1998; <a href=https://arxiv.org/abs/1606.04474>Andrychowicz et al., 2016</a>; <a href=https://arxiv.org/abs/1611.03824>Chen et al., 2016</a>; <a href="https://www.youtube.com/watch?v=x1kf4Zojtb0">de Freitas, 2016</a>; <a href=https://arxiv.org/abs/1611.02779>Duan et al., 2016</a>; <a href=https://arxiv.org/abs/1604.00289>Lake et al., 2016</a>; <a href=https://arxiv.org/abs/1611.05763>Wang et al., 2016</a>) is the acquisition of skills and inductive biases that facilitate future learning. The scenarios considered in particular are ones where a more general and slower learning process produces a faster, more specialized one. An example is biological evolution producing efficient learners such as human beings.</p></li><li><h3 id=tests>Tests<a hidden class=anchor aria-hidden=true href=#tests>#</a></h3><ul><li>Learning to play Atari video games is an area that has seen some remarkable recent successes, including in transfer learning (<a href=https://arxiv.org/abs/1511.06342>Parisotto et al., 2016</a>). However, there is so far no system that first learns to play video games, then is capable of learning a new game, as humans can, from a few minutes of play (<a href=https://arxiv.org/abs/1604.00289>Lake et al., 2016</a>).</li></ul></li></ul><h2 id=knowing-when-you-dontknow>Knowing when you don’t know<a hidden class=anchor aria-hidden=true href=#knowing-when-you-dontknow>#</a></h2><ul><li>While uncertainty is modeled differently by different learning algorithms, it seems to be true in general that current artificial systems are not nearly as good as humans at “knowing when they don’t know.” An example are deep neural networks that achieve state-of-the-art accuracy on image recognition but assign 99.99% confidence to the presence of objects in images completely unrecognizable to humans (<a href=http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.pdf>Nguyen et al., 2015</a>).</li></ul><p>Human performance on confidence estimation would include</p><ul><li><p>In induction tasks, like program induction or sequence completion, knowing when the provided examples are insufficient for induction (multiple reasonable hypotheses could account for them)</p></li><li><p>In speech recognition, knowing when an utterance has not been interpreted reliably</p></li><li><p>In visual tasks such as pedestrian detection, knowing when a part of the image has not been analyzed reliably</p></li><li><h3 id=tests-1>Tests<a hidden class=anchor aria-hidden=true href=#tests-1>#</a></h3><ul><li><p>A speech recognizer can be compared against a human baseline, measuring the ratio of the average confidence to the confidence on examples where recognition fails.</p></li><li><p>The confidence of image recognition systems can be tested on generated adversarial examples.</p></li></ul></li><li></li></ul><h2 id=learning-throughaction>Learning through action<a hidden class=anchor aria-hidden=true href=#learning-throughaction>#</a></h2><ul><li>Human infants are known to learn about the world through experiments, observing the effects of their own actions (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.470.5304&amp;rep=rep1&amp;type=pdf">Smith and Gasser, 2005</a>; <a href="https://www.youtube.com/watch?v=QaF2kkez5XU">Malik, 2015</a>). This seems to apply both to higher-level cognition and perception. Animal experiments have confirmed that the ability to initiate movement is crucial to perceptual development (<a href=http://www.miamikillianhs.com/ourpages/auto/2011/9/28/55941483/AICS%20Psychol%20%20Held_%20Movement-Produced%20StimulationHeld-1963.pdf>Held and Hein, 1963</a>) and some recent progress has been made on using motion in learning visual perception (<a href=http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Agrawal_Learning_to_See_ICCV_2015_paper.pdf>Agrawal et al., 2015</a>). In (<a href=https://arxiv.org/abs/1606.07419>Agrawal et al., 2016</a>), a robot learns to predict the effects of a poking action.</li></ul><p>“Learning through action” thus encompasses several areas, including</p><ul><li><p>Active learning, where the agent selects the training examples most likely to be instructive</p></li><li><p>Undertaking epistemological actions, i.e., activities aimed primarily at gathering information</p></li><li><p>Learning to perceive through action</p></li><li><p>Learning about causal relationships through action</p></li></ul><mark>Perhaps most importantly, for artificial systems, learning the causal structure of the world through experimentation is still an open problem.</mark><ul><li><h3 id=tests-2>Tests<a hidden class=anchor aria-hidden=true href=#tests-2>#</a></h3><ul><li><p>For learning through action, it is natural to consider problems of motor manipulation where in addition to the immediate effects of the agent’s actions, secondary effects must be considered as well.</p></li><li><p>Learning to play billiards: An agent with little prior knowledge and no fixed training data is allowed to explore a real or virtual billiard table and should learn to play billiards well.</p></li></ul></li></ul><h2 id=transfer-flexibility>Transfer flexibility<a hidden class=anchor aria-hidden=true href=#transfer-flexibility>#</a></h2></div><hr><aside><h4>No linked references</h4></aside><br><aside class=related></aside><footer class=post-footer><ul class=post-tags><li><a href=https://shrimansoft.github.io/knowledge-base/tags/reinforcement-learning/>Reinforcement Learning</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://shrimansoft.github.io/knowledge-base/>Shriman Keshri</a></span><br><span>Powered by
<a href=https://github.com/sawhney17/logseq-schrodinger rel=noopener target=_blank>Logseq Schrödinger</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>