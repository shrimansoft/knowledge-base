<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Markove decision process | Shriman Keshri</title>
<meta name=keywords content><meta name=description content="id:: 6480345f-4734-4296-ab89-5203b50f3c33 RL interaction Dynamics Markove decision process is a mathematical model.
When we fix the policy, the above process becomes automatic. It will just run and run.
Because of a lack of knowledge, we use probability in this. #randomness
The above interaction dynamics can be studied using the MDP. So here the MDP comes in picture.
The MDP is how the agent sees his interaction with the world.
Let me elaborate on this and give a picture of where this idea will fit in the scheme of World view."><meta name=author content="Shriman Keshri"><link rel=canonical href=https://shrimansoft.github.io/knowledge-base/pages/markove-decision-process/><link crossorigin=anonymous href=https://shrimansoft.github.io/knowledge-base/assets/css/stylesheet.min.49b1c3611e5e823e974d0b9b3e1101617fce26c004757161abb1a1e4af3ff85c.css integrity="sha256-SbHDYR5egj6XTQubPhEBYX/OJsAEdXFhq7Gh5K8/+Fw=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=https://shrimansoft.github.io/knowledge-base/assets/js/highlight.min.5b9ae0304f93db6cc493f51846f012428af399c614b4f2fbdb7fa59dd4d5ef5b.js integrity="sha256-W5rgME+T22zEk/UYRvASQorzmcYUtPL723+lndTV71s=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://shrimansoft.github.io/knowledge-base/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://shrimansoft.github.io/knowledge-base/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://shrimansoft.github.io/knowledge-base/favicon-32x32.png><link rel=apple-touch-icon href=https://shrimansoft.github.io/knowledge-base/apple-touch-icon.png><link rel=mask-icon href=https://shrimansoft.github.io/knowledge-base/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.120.4"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Markove decision process"><meta property="og:description" content="id:: 6480345f-4734-4296-ab89-5203b50f3c33 RL interaction Dynamics Markove decision process is a mathematical model.
When we fix the policy, the above process becomes automatic. It will just run and run.
Because of a lack of knowledge, we use probability in this. #randomness
The above interaction dynamics can be studied using the MDP. So here the MDP comes in picture.
The MDP is how the agent sees his interaction with the world.
Let me elaborate on this and give a picture of where this idea will fit in the scheme of World view."><meta property="og:type" content="article"><meta property="og:url" content="https://shrimansoft.github.io/knowledge-base/pages/markove-decision-process/"><meta property="article:section" content="pages"><meta property="article:published_time" content="2023-12-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-12-04T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Markove decision process"><meta name=twitter:description content="id:: 6480345f-4734-4296-ab89-5203b50f3c33 RL interaction Dynamics Markove decision process is a mathematical model.
When we fix the policy, the above process becomes automatic. It will just run and run.
Because of a lack of knowledge, we use probability in this. #randomness
The above interaction dynamics can be studied using the MDP. So here the MDP comes in picture.
The MDP is how the agent sees his interaction with the world.
Let me elaborate on this and give a picture of where this idea will fit in the scheme of World view."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Pages","item":"https://shrimansoft.github.io/knowledge-base/pages/"},{"@type":"ListItem","position":3,"name":"Markove decision process","item":"https://shrimansoft.github.io/knowledge-base/pages/markove-decision-process/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Markove decision process","name":"Markove decision process","description":"id:: 6480345f-4734-4296-ab89-5203b50f3c33 RL interaction Dynamics Markove decision process is a mathematical model.\nWhen we fix the policy, the above process becomes automatic. It will just run and run.\nBecause of a lack of knowledge, we use probability in this. #randomness\nThe above interaction dynamics can be studied using the MDP. So here the MDP comes in picture.\nThe MDP is how the agent sees his interaction with the world.\nLet me elaborate on this and give a picture of where this idea will fit in the scheme of World view.","keywords":[],"articleBody":"id:: 6480345f-4734-4296-ab89-5203b50f3c33 RL interaction Dynamics Markove decision process is a mathematical model.\nWhen we fix the policy, the above process becomes automatic. It will just run and run.\nBecause of a lack of knowledge, we use probability in this. #randomness\nThe above interaction dynamics can be studied using the MDP. So here the MDP comes in picture.\nThe MDP is how the agent sees his interaction with the world.\nLet me elaborate on this and give a picture of where this idea will fit in the scheme of World view. So, we start our story with two players one is our robot $R$ and is the enlivenment where the robot acts say $E$. Now there is up to how is thinking about all these interactions. Here the person who is thinking about the interaction says $Thinker$.\nThe question is, where is this MDP? The MDP is modelled by the $Thinker$ and placed in side the code/brain of $R$. What this MDP is representing. Its representing the dynamics of the interaction between $R$ and $E$. alring.\nSo, what we get from this? that there is complete interaction dynamics which is going in side the head of $R$ which is some partial representaiton of the real dynamics which is going to happen on real.\nvery good. now we can see there is two interaction flow one is on real-world where real robot build of moters, battery and chipes is action on the environment. and the other one which is action in the head of the $R$.\nbut the point the concern me is this. The model of the is not build by the $R$ or some code in side the $R$. its someting we crated and placed in the $R$ . But in case of us we human we found the interaction dynamics by our own.\nThe question this leades me to are: How we know that we are not the part of this enviroment? how do we com and get awear the table is not the same as pen. how do we know how to interacte with them and how the concept of tool which say make us not only awear of how we interact in this world. but also. how on opject in the world interact with the other object in the world.\nRemark: at the above line where I write “object” is the world in the World view.\nLet me elaborate on this and give a picture of where this idea will fit in the scheme of World view. So, we start our story with two players one is our robot $R$ and is the enlivenment where the robot acts say $E$. Now there is up to how is thinking about all these interactions. Here the person who is thinking about the interaction says $Thinker$.\nThe question is, where is this MDP? The MDP is modelled by the $Thinker$ and placed in side the code/brain of $R$. What this MDP is representing. Its representing the dynamics of the interaction between $R$ and $E$. alring.\nSo, what we get from this? that there is complete interaction dynamics which is going in side the head of $R$ which is some partial representaiton of the real dynamics which is going to happen on real.\nvery good. now we can see there is two interaction flow one is on real-world where real robot build of motors, battery and chips is action on the environment. and the other one which is action in the head of the $R$.\nbut the point the concern for me is this. The model of this is not build by the $R$ or some code in side the $R$. its something we crated and placed in the $R$ . But in case of us we human we found the interaction dynamics by our own.\nThe question this leads me to are: How we know that we are not the part of this environment? how do we come and get aware that the table is not the same as pen. how do we know interaction with them and how the concept of tool which say make us not only aware of how we interact in this world. but also. how on object in the world interact with the other object in the world.\nRemark: at the above line where I write “object” is the world in the World view.\nBy looking the the above challenge. I am going to propose an experiment challenge: The Challenge: given a $agent$ \u003c-\u003e $world$ interaction. where the world is consist of two worlds say $world$ := $world 1$ \u003c-\u003e $world 2$. the challenge the agent has is to find the MDP which represent the $agent$ \u003c-\u003e $world$ to The MDP which is representing the $agent$ \u003c-\u003e $world 1$ \u003c-\u003e $world 2$.\nOK the next problem is how can we write the MDP with three interacting or three world. where each of them.\nBy looking the the above chalange. I am going to propose an experiment chalange: The Chalange: given a $agent$ \u003c-\u003e $world$ interaction. where the world is consit of two worlds say $world$ := $world1$ \u003c-\u003e $world2$. the chanlage the agent has is he need to find the MDP which is representitng the $agent$ \u003c-\u003e $world$ to The MDP which is representing the $agent$ \u003c-\u003e $world1$ \u003c-\u003e $world2$.\nOK the next problam is how can we write the MDP with three ingridinan or three world. where each of them.\nTriangular MDP This MPP is the generalisation of the interaction with the world that Is and built. When you think about a bike, you don’t see all the nuts and bolts. You see how it operates and how its state changes when you perform some action. the Idea of a bike is the MDP of the Bike in our head.\nThis is cool. When I say I know something i.e. I know the behaviour of that thing and how it changes based on my action. I know the dynamic rules of that person or thing. #knowing\nRemark: Most real scenarios are unlikely to be Markov.\nQuestion: How are we dealing with this? this question has two parts. one, what are the technic we have in computer science to deal with this. another one is how we humans are dealing with this.\nAnswer: I don’t this will have a clear and clean answer. but let me mention some points.\nMemory of the past states that we use to handle this. we can have Indexes That depends on the past state. Through this we can gather some particular information for the past states. There is a quote that I read in the course of Second Brain that “we humans are good at recognising then recalling”. So, what this is saying this how we retreat information from our Memory. #Memory ","wordCount":"1133","inLanguage":"en","datePublished":"2023-12-02T00:00:00Z","dateModified":"2023-12-04T00:00:00Z","author":{"@type":"Person","name":"Shriman Keshri"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://shrimansoft.github.io/knowledge-base/pages/markove-decision-process/"},"publisher":{"@type":"Organization","name":"Shriman Keshri","logo":{"@type":"ImageObject","url":"https://shrimansoft.github.io/knowledge-base/favicon.ico"}}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-00000XXXXX"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),-gtag("config","G-00000XXXXX")</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://shrimansoft.github.io/knowledge-base accesskey=h title="Shriman Keshri (Alt + H)">Shriman Keshri</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://shrimansoft.github.io/knowledge-base/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://shrimansoft.github.io/knowledge-base/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://shrimansoft.github.io/knowledge-base/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://shrimansoft.github.io/knowledge-base/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><h1 class=post-title>Markove decision process</h1><div class=post-meta><span title='2023-12-02 00:00:00 +0000 UTC'>December 2, 2023</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Shriman Keshri</div></header><div class=post-content><p>id:: 6480345f-4734-4296-ab89-5203b50f3c33<div class=center><img loading=lazy src=https://shrimansoft.github.io/knowledge-base/assets/image_1679864939461_0.png alt=image.png></div></p><div class=center>RL interaction Dynamics</div><p>Markove decision process is a mathematical model.</p><blockquote><p>When we fix the <strong>policy</strong>, the above process becomes automatic. It will just run and run.</p></blockquote><p>Because of a <strong>lack of knowledge</strong>, we use probability in this. #randomness</p><p>The above <a href="logseq://graph/game?block-id=6480345f-4734-4296-ab89-5203b50f3c33">interaction dynamics</a> can be studied using the MDP. So here the MDP comes in picture.</p><p>The MDP is how the agent sees his interaction with the world.</p><ul><li>Let me elaborate on this and give a picture of where this idea will fit in the scheme of World view. So, we start our story with two players one is our robot $R$ and is the enlivenment where the robot acts say $E$.</li></ul><p>Now there is up to how is thinking about all these interactions. Here the person who is thinking about the interaction says $Thinker$.</p><p>The question is, where is this MDP? The MDP is modelled by the $Thinker$ and placed in side the code/brain of $R$. What this MDP is representing. Its representing the dynamics of the interaction between $R$ and $E$. alring.</p><p>So, what we get from this? that there is complete interaction dynamics which is going in side the head of $R$ which is some partial representaiton of the real dynamics which is going to happen on real.</p><p>very good. now we can see there is two interaction flow one is on real-world where real robot build of moters, battery and chipes is action on the environment. and the other one which is action in the head of the $R$.</p><p>but the point the concern me is this. The model of the is not build by the $R$ or some code in side the $R$. its someting we crated and placed in the $R$ . But in case of us we human we found the interaction dynamics by our own.</p><p>The question this leades me to are:
How we know that we are not the part of this enviroment? how do we com and get awear the table is not the same as pen. how do we know how to interacte with them and how the concept of tool which say make us not only awear of how we interact in this world. but also. how on opject in the world interact with the other object in the world.</p><p>Remark: at the above line where I write &ldquo;object&rdquo; is the world in the World view.</p><ul><li>Let me elaborate on this and give a picture of where this idea will fit in the scheme of World view. So, we start our story with two players one is our robot $R$ and is the enlivenment where the robot acts say $E$.</li></ul><p>Now there is up to how is thinking about all these interactions. Here the person who is thinking about the interaction says $Thinker$.</p><p>The question is, where is this MDP? The MDP is modelled by the $Thinker$ and placed in side the code/brain of $R$. What this MDP is representing. Its representing the dynamics of the interaction between $R$ and $E$. alring.</p><p>So, what we get from this? that there is complete interaction dynamics which is going in side the head of $R$ which is some partial representaiton of the real dynamics which is going to happen on real.</p><p>very good. now we can see there is two interaction flow one is on real-world where real robot build of motors, battery and chips is action on the environment. and the other one which is action in the head of the $R$.</p><p>but the point the concern for me is this. The model of this is not build by the $R$ or some code in side the $R$. its something we crated and placed in the $R$ . But in case of us we human we found the interaction dynamics by our own.</p><p>The question this leads me to are:
How we know that we are not the part of this environment? how do we come and get aware that the table is not the same as pen. how do we know interaction with them and how the concept of tool which say make us not only aware of how we interact in this world. but also. how on object in the world interact with the other object in the world.</p><p>Remark: at the above line where I write &ldquo;object&rdquo; is the world in the World view.</p><ul><li>By looking the the above challenge. I am going to propose an experiment challenge:</li></ul><p><strong>The Challenge</strong>: given a $agent$ &lt;-> $world$ interaction. where the world is consist of two worlds say $world$ := $world 1$ &lt;-> $world 2$. the challenge the agent has is to find the MDP which represent the $agent$ &lt;-> $world$ to The MDP which is representing the $agent$ &lt;-> $world 1$ &lt;-> $world 2$.</p><p>OK the next problem is how can we write the MDP with three interacting or three world. where each of them.</p><ul><li>By looking the the above chalange. I am going to propose an experiment chalange:</li></ul><p><strong>The Chalange</strong>: given a $agent$ &lt;-> $world$ interaction. where the world is consit of two worlds say $world$ := $world1$ &lt;-> $world2$. the chanlage the agent has is he need to find the MDP which is representitng the $agent$ &lt;-> $world$ to The MDP which is representing the $agent$ &lt;-> $world1$ &lt;-> $world2$.</p><p>OK the next problam is how can we write the MDP with three ingridinan or three world. where each of them.</p><ul><li>Triangular MDP</li></ul><p>This MPP is the generalisation of the interaction with the world that Is and built. When you think about a bike, you don&rsquo;t see all the nuts and bolts. You see how it operates and how its state changes when you perform some action. the Idea of a bike is the MDP of the Bike in our head.</p><p>This is cool. When I say I know something i.e. I know the behaviour of that thing and how it changes based on my action. I know the dynamic rules of that person or thing. #knowing</p><p><strong>Remark:</strong> Most real scenarios are unlikely to be Markov.</p><ul><li><p><strong>Question:</strong> How are we dealing with this? this question has two parts. one, what are the technic we have in computer science to deal with this. another one is how we humans are dealing with this.</p></li><li><p><strong>Answer:</strong> I don&rsquo;t this will have a clear and clean answer. but let me mention some points.</p></li></ul><ol><li><strong>Memory</strong> of the past states that we use to handle this.</li><li>we can have Indexes That depends on the past state. Through this we can gather some particular information for the past states.</li><li>There is a quote that I read in the course of Second Brain that &ldquo;we humans are good at recognising then recalling&rdquo;. So, what this is saying this how we retreat information from our Memory. #Memory</li></ol><ul><li></li><li></li></ul></div><hr><aside><h3>Linked References</h3><div class=backlinks><ul><p class=capitalize><a style=color:var(--link) href=https://shrimansoft.github.io/knowledge-base/pages/generalisation-of-reinforcement-learning/>Generalisation of reinforcement learning</a></p><p class=capitalize><a style=color:var(--link) href=https://shrimansoft.github.io/knowledge-base/pages/reinforcement-learning/>Reinforcement Learning</a></p></ul></div></aside><br><aside class=related></aside><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://shrimansoft.github.io/knowledge-base>Shriman Keshri</a></span><br><span>Powered by
<a href=https://github.com/sawhney17/logseq-schrodinger rel=noopener target=_blank>Logseq Schrödinger</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>